{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5: Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: mean, standard deviation of a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard preamble\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats # notice that we're introducing a new library here\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about what's available in the stats module, have a look here: https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "\n",
    "Read randomly sampled data from a file, histogram it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = np.loadtxt('sample.dat',unpack=True)\n",
    "print('Number of samples =',len(x1))\n",
    "\n",
    "bins = np.arange(-0.5,10.5,1)\n",
    "n, b, p = plt.hist(x1, bins)\n",
    "\n",
    "# various measures of \"average value\":\n",
    "print ('Mean = {0:3.1f}'.format(np.mean(x1)))\n",
    "print ('Median = {0:3.1f}'.format(np.median(x1)))\n",
    "print ('Mode = {0:3.1f}'.format(0.5*(bins[np.argmax(n)]+bins[np.argmax(n)+1])))\n",
    "mode, count = sp.stats.mode(x1)\n",
    "print('Mode from SciPy = {0:3.1f}'.format(mode[0]))\n",
    "\n",
    "# measure of the spread\n",
    "print ('Standard deviation = {0:3.1f}'.format(np.std(x1,ddof=1)))  # unbiased with ddof=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the correlation between two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x1,x2)\n",
    "corr = np.corrcoef(x1,x2)\n",
    "print('Pearson correlation coefficient = {0:5.3f}'.format(corr[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of data and compute mean and variance\n",
    "# This creates an array of 100 elements, \n",
    "# gaussian-distributed with mean of 200 and RMS of 25\n",
    "mu = 100\n",
    "sigma = 25\n",
    "x = mu + sigma*np.random.randn(10000)\n",
    "print (x[0:10])\n",
    "n, bins, patches = plt.hist(x, 20, density=True)  # NB: new keyword in Matplotlib 3.3 \n",
    "\n",
    "# various measures of \"average value\":\n",
    "print ('Mean = {0:5.2f}'.format(np.mean(x)))\n",
    "print ('Median = {0:5.2f}'.format(np.median(x)))\n",
    "print ('Mode = {0:5.2f}'.format(0.5*(bins[np.argmax(n)]+bins[np.argmax(n)+1])))\n",
    "\n",
    "# measure of the spread\n",
    "print ('Standard deviation = {0:5.1f}'.format(np.std(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a Poisson process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.loadtxt('sample_merged.dat',unpack=True)\n",
    "#x = np.concatenate((x1,x2))\n",
    "#x = x1.copy()\n",
    "\n",
    "N = len(x)\n",
    "print('Number of samples =',N)\n",
    "bins = np.arange(-0.5,10.5,1)\n",
    "n, b, p = plt.hist(x, bins)\n",
    "expected = 0.1*N*np.ones(len(n))\n",
    "# loop over all entries in the histogram, compute chi^2 assuming \n",
    "# Poisson errors for each bin\n",
    "\n",
    "en = np.zeros(len(n))\n",
    "for i in range(0,10):\n",
    "    value = n[i]\n",
    "    error = np.sqrt(value)\n",
    "    if value == 0:\n",
    "        error = 1\n",
    "    en[i] = error\n",
    "    \n",
    "# plot the distribution with error bars\n",
    "x = np.arange(0,10)\n",
    "plt.errorbar(x, n, xerr=0, yerr=en, fmt='bo')\n",
    "plt.xlabel('Number')\n",
    "plt.ylabel('Samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central limit theorem\n",
    "\n",
    "Central limit theorem states that if you have any PDF with mean $\\mu$ and variance $\\sigma^2$, and you draw $N$ samples $\\{ x_i \\},\\ i=[1..N]$ from the distribution, the PDF of the <b>sample mean</b>\n",
    "$$\n",
    "\\mu_s = \\frac{1}{N}\\sum_{i=1}^N x_i\n",
    "$$\n",
    "approaches Gaussian with mean $\\langle \\mu_s\\rangle = \\mu$ and variance \n",
    "$\\sigma_s^2 = \\sigma^2/N$ when $N\\to\\infty$\n",
    "\n",
    "Let's look at the example. Take the data collected in class:\n",
    "\n",
    "$$N=17$$\n",
    "$$f(x_i) = \\frac{1}{10}, x_i\\in[0..9]$$\n",
    "\n",
    "As an exercise for the reader, prove analytically that \n",
    "$$\\sigma = 10/\\sqrt{12} = 2.89$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean and sigma of the parent distribution\n",
    "\n",
    "mu = 0\n",
    "sigma = 0\n",
    "prob = 1./10.\n",
    "for i in range(0,10):\n",
    "    mu += prob*i\n",
    "print('mu = {0:4.2f}'.format(mu))\n",
    "\n",
    "for i in range(0,10):\n",
    "    sigma += prob*(i-mu)**2\n",
    "sigma = np.sqrt(sigma)\n",
    "print('sigma = {0:4.2f}'.format(sigma))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now draw 17 random <i>integer</i> numbers uniformly distributed between 0 and 9 inclusively, and compute the mean of the distribution. Let's define a function for that, as we would be calling it a large number of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawN(N):\n",
    "    samples = np.random.randint(0,10,N)\n",
    "    return np.mean(samples)\n",
    "\n",
    "N = 17\n",
    "print(drawN(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a population of the means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ndraws = 10000\n",
    "x = np.empty(Ndraws)\n",
    "N = 17\n",
    "for i in range(Ndraws):\n",
    "    x[i] = drawN(N)\n",
    "    \n",
    "n, bins, patches = plt.hist(x, 20, density=False)\n",
    "\n",
    "# various measures of \"average value\":\n",
    "print ('Mean = {0:5.3f}'.format(np.mean(x)))\n",
    "print ('Median = {0:5.3f}'.format(np.median(x)))\n",
    "print ('Mode = {0:5.3f}'.format(0.5*(bins[np.argmax(n)]+bins[np.argmax(n)+1])))\n",
    "\n",
    "# measure of the spread\n",
    "print ('Standard deviation = {0:5.3f}'.format(np.std(x)))\n",
    "\n",
    "# check\n",
    "print('Predicted value of standard deviation = {0:5.3f}'.format(sigma/np.sqrt(N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fitting\n",
    "\n",
    "The simplest technique to describe is least-squares fitting (see lecture notes). Usually you use the least-squares fit if you have a graph (i.e. a set of data points $y_i(x_i)$), you want to describe it in terms of a model $y(x;\\theta)$, where parameters $\\theta$ are unknown. You fit to determine the values of $\\theta$ and (hopefully) their uncertainties.  \n",
    "\n",
    "There are two standard cases where least-squares method is applicable:\n",
    "1. You know errors for each data point $\\sigma_i$ and you know that those errors are Gaussian. In this case, you minimize $\\chi^2=\\sum \\left(\\frac{y_i - y(x_i;\\theta)}{\\sigma_i}\\right)^2$. The value of the $\\chi^2_{\\min}$ can be interpreted as a goodness-of-fit, and the errors on parameters $\\theta$ have probabilistic interpretation\n",
    "1. You know that the errors are Gaussian and are the same for each data point, but you do not know their magnitude. In this case, you would minimize the sum of squares: $\\mathcal{S} = \\sum \\left(y_i - y(x_i;\\theta)\\right)^2$. In this case the value of $\\mathcal{S}$ can be used to compute the errors $\\sigma_i$ for each data point, and the errors on $\\theta$ have probabilistic definition, but you lose information about the goodness of fit\n",
    "1. If the errors are not known to be Gaussian, then the least square method is not useful to estimate uncertainties or the goodness of fit. It is also not guaranteed to be unbiased or most efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial fit\n",
    "\n",
    "Let's use the scipy.optimize module: https://docs.scipy.org/doc/scipy/reference/optimize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as fitter\n",
    "\n",
    "\n",
    "# Generate artificial data = straight line with a=0 and b=1\n",
    "# plus some noise.\n",
    "a0 = 0\n",
    "b0 = 1\n",
    "xdata = np.array([0.0,1.0,2.0,3.0,4.0,5.0])  # Voltage (V)\n",
    "ydata = np.array([0.1,0.9,2.2,2.8,3.9,5.1])  # current (A)\n",
    "sigma = np.array([1.0,1.0,1.0,1.0,1.0,1.0])*0.2  # 0.2 A error in current\n",
    "\n",
    "# plot it\n",
    "plt.errorbar(xdata, ydata, xerr=0, yerr=sigma, fmt='o')\n",
    "plt.xlim(-1,6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear function\n",
    "def model(x, a, b):\n",
    "    return a + b*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitter needs a good initial guess of parameters (more on that later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial guess.\n",
    "par0    = np.array([0.0, 1.0])\n",
    "par, cov = fitter.curve_fit(model, xdata, ydata, p0=par0, sigma=sigma, absolute_sigma=True)\n",
    "print (par)\n",
    "print (cov)\n",
    "\n",
    "# decode it now\n",
    "print ('a={0:6.3f}+/-{1:5.3f}'.format(par[0],np.sqrt(cov[0,0])))\n",
    "print ('b={0:6.3f}+/-{1:5.3f}'.format(par[1],np.sqrt(cov[1,1])))\n",
    "\n",
    "corr = cov.copy() # copy shape\n",
    "# compute correlation matrix\n",
    "for i in range(len(par)):\n",
    "    for j in range(len(par)):\n",
    "        corr[i,j] = cov[i,j]/np.sqrt(cov[i,i]*cov[j,j])\n",
    "        \n",
    "print (corr)\n",
    "\n",
    "# compute reduced chi2\n",
    "chi_squared = np.sum(((model(xdata, *par)-ydata)/sigma)**2)\n",
    "reduced_chi_squared = (chi_squared)/(len(xdata)-len(par))\n",
    "print ('chi^2 = {0:5.2f} for d.f.={1:d}'.format(chi_squared,len(xdata)-len(par)))\n",
    "print ('chi^2/d.f.={0:5.2e}'.format(reduced_chi_squared))\n",
    "\n",
    "# overlay plot over data\n",
    "plt.errorbar(xdata, ydata, xerr=0, yerr=sigma, fmt='o')\n",
    "plt.xlim(-1,6)\n",
    "xfit = np.linspace(0,5,50)\n",
    "plt.plot(xfit,model(xfit,par[0],par[1]),'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same, but use a second order polynomial as our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sqrt\n",
    "\n",
    "# quadratic function\n",
    "def model2(x, a, b, c):\n",
    "    return a + b*x + c*x*x\n",
    "\n",
    "par0    = np.array([0.0, 1.0, 0.0])\n",
    "par, cov = fitter.curve_fit(model2, xdata, ydata, p0=par0, sigma=sigma, absolute_sigma=True)\n",
    "print (par)\n",
    "print (cov)\n",
    "\n",
    "# decode it now\n",
    "print ('a={0:6.3f}+/-{1:5.3f}'.format(par[0],sqrt(cov[0,0])))\n",
    "print ('b={0:6.3f}+/-{1:5.3f}'.format(par[1],sqrt(cov[1,1])))\n",
    "print ('c={0:6.3f}+/-{1:5.3f}'.format(par[2],sqrt(cov[2,2])))\n",
    "\n",
    "corr = cov.copy() # copy shape\n",
    "# compute correlation matrix\n",
    "for i in range(len(par)):\n",
    "    for j in range(len(par)):\n",
    "        corr[i,j] = cov[i,j]/sqrt(cov[i,i]*cov[j,j])\n",
    "        \n",
    "print (corr)\n",
    "\n",
    "# compute reduced chi2\n",
    "chi_squared = np.sum(((model2(xdata, *par)-ydata)/sigma)**2)\n",
    "reduced_chi_squared = (chi_squared)/(len(xdata)-len(par))\n",
    "print ('chi^2 = {0:5.2f}'.format(chi_squared))\n",
    "print ('chi^2/d.f.={0:5.2f}'.format(reduced_chi_squared))\n",
    "\n",
    "# overlay plot over data\n",
    "plt.errorbar(xdata, ydata, xerr=0, yerr=sigma, fmt='o')\n",
    "plt.xlim(-1,6)\n",
    "xfit = np.linspace(0,5,50)\n",
    "plt.plot(xfit,model2(xfit,par[0],par[1], par[2]),'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extend this to use a N-th order polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nth-order poly function\n",
    "def modelN(*arg):\n",
    "    x = arg[0]\n",
    "    N = len(arg)-1\n",
    "    sum = arg[1]\n",
    "    for i in range(1,N):\n",
    "        sum += arg[i+1]*x**i\n",
    "    return sum\n",
    "\n",
    "N=2# poly degree\n",
    "par0    = np.zeros(N+1)\n",
    "par[1]=1\n",
    "par, cov = fitter.curve_fit(modelN, xdata, ydata, p0=par0, sigma=sigma,absolute_sigma=True)\n",
    "print (par)\n",
    "print (cov)\n",
    "\n",
    "corr = cov.copy() # copy shape\n",
    "# compute correlation matrix\n",
    "for i in range(len(par)):\n",
    "    for j in range(len(par)):\n",
    "        corr[i,j] = cov[i,j]/np.sqrt(cov[i,i]*cov[j,j])\n",
    "        \n",
    "print (corr)\n",
    "\n",
    "# compute reduced chi2\n",
    "chi_squared = np.sum(((modelN(xdata, *par)-ydata)/sigma)**2)\n",
    "reduced_chi_squared = (chi_squared)/(len(xdata)-len(par))\n",
    "print ('chi^2 = {0:5.2f}'.format(chi_squared))\n",
    "print ('chi^2/d.f.={0:5.2f}'.format(reduced_chi_squared))\n",
    "\n",
    "# overlay plot over data\n",
    "plt.errorbar(xdata, ydata, xerr=0, yerr=sigma, fmt='o')\n",
    "plt.xlim(-1,6)\n",
    "xfit = np.linspace(0,5,50)\n",
    "plt.plot(xfit,modelN(xfit,*par),'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveat: correlations\n",
    "\n",
    "You may notice that the polynomial parameters are highly correlated. You can reduce them by using *Chebychev polynomials*. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear function\n",
    "def modelC(x, a, b):\n",
    "    return a + b*(x-2.5)\n",
    "\n",
    "# Initial guess.\n",
    "par0    = np.array([0.0, 1.0])\n",
    "par, cov = fitter.curve_fit(modelC, xdata, ydata, par0, sigma)\n",
    "print (par)\n",
    "print (cov)\n",
    "\n",
    "# decode it now\n",
    "print ('a={0:6.3f}+/-{1:5.3f}'.format(par[0],sqrt(cov[0,0])))\n",
    "print ('b={0:6.3f}+/-{1:5.3f}'.format(par[1],sqrt(cov[1,1])))\n",
    "\n",
    "corr = cov.copy() # copy shape\n",
    "# compute correlation matrix\n",
    "for i in range(len(par)):\n",
    "    for j in range(len(par)):\n",
    "        corr[i,j] = cov[i,j]/sqrt(cov[i,i]*cov[j,j])\n",
    "        \n",
    "print (corr)\n",
    "\n",
    "# compute reduced chi2\n",
    "chi_squared = np.sum(((modelC(xdata, *par)-ydata)/sigma)**2)\n",
    "reduced_chi_squared = (chi_squared)/(len(xdata)-len(par))\n",
    "print ('chi^2 = {0:5.2f}'.format(chi_squared))\n",
    "print ('chi^2/d.f.={0:5.2f}'.format(reduced_chi_squared))\n",
    "\n",
    "# overlay plot over data\n",
    "plt.errorbar(xdata, ydata, xerr=0, yerr=sigma, fmt='o')\n",
    "plt.xlim(-1,6)\n",
    "xfit = np.linspace(0,5,50)\n",
    "plt.plot(xfit,modelC(xfit,par[0],par[1]),'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning: Lack of robustness\n",
    "Gradient methods such as *Levenburg-Marquardt* used by *leastsq/curve_fit* are not robust and simply run into the nearest local minimum. Therefore, it is important to provide the initial set of values that are \"close enough\". Here is the demonstration below (courtesy https://python4mpia.github.io/fitting_data/least-squares-fitting.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy,math\n",
    "import scipy.optimize as optimization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Chose a model that will create bimodality.\n",
    "def func(x, a, b):\n",
    "    return a + b*b*x  # Term b*b will create bimodality.\n",
    "\n",
    "# Create toy data for curve_fit.\n",
    "xdata = numpy.array([0.0,1.0,2.0,3.0,4.0,5.0])\n",
    "ydata = numpy.array([0.1,0.9,2.2,2.8,3.9,5.1])\n",
    "sigma = numpy.array([1.0,1.0,1.0,1.0,1.0,1.0])\n",
    "\n",
    "# Compute chi-square manifold.\n",
    "Steps = 101  # grid size\n",
    "Chi2Manifold = numpy.zeros([Steps,Steps])  # allocate grid\n",
    "amin = -7.0  # minimal value of a covered by grid\n",
    "amax = +5.0  # maximal value of a covered by grid\n",
    "bmin = -4.0  # minimal value of b covered by grid\n",
    "bmax = +4.0  # maximal value of b covered by grid\n",
    "for s1 in range(Steps):\n",
    "    for s2 in range(Steps):\n",
    "        # Current values of (a,b) at grid position (s1,s2).\n",
    "        a = amin + (amax - amin)*float(s1)/(Steps-1)\n",
    "        b = bmin + (bmax - bmin)*float(s2)/(Steps-1)\n",
    "        # Evaluate chi-squared.\n",
    "        chi2 = 0.0\n",
    "        for n in range(len(xdata)):\n",
    "            residual = (ydata[n] - func(xdata[n], a, b))/sigma[n]\n",
    "            chi2 = chi2 + residual*residual\n",
    "        Chi2Manifold[Steps-1-s2,s1] = chi2  # write result to grid.\n",
    "\n",
    "# Plot grid.\n",
    "plt.figure(1, figsize=(8,4.5))\n",
    "plt.subplots_adjust(left=0.09, bottom=0.09, top=0.97, right=0.99)\n",
    "# Plot chi-square manifold.\n",
    "image = plt.imshow(Chi2Manifold, vmax=50.0,\n",
    "              extent=[amin, amax, bmin, bmax])\n",
    "# Plot where curve-fit is going to for a couple of initial guesses.\n",
    "for a_initial in -6.0, -4.0, -2.0, 0.0, 2.0, 4.0:\n",
    "    # Initial guess.\n",
    "    x0   = numpy.array([a_initial, -3.5])\n",
    "    xFit = optimization.curve_fit(func, xdata, ydata, x0, sigma)[0]\n",
    "    plt.plot([x0[0], xFit[0]], [x0[1], xFit[1]], 'o-', ms=4,\n",
    "                 markeredgewidth=0, lw=2, color='orange')\n",
    "    \n",
    "for a_initial in -6.0, -4.0, -2.0, 0.0, 2.0, 4.0:\n",
    "    # Initial guess.\n",
    "    x0   = numpy.array([a_initial, 3.5])\n",
    "    xFit = optimization.curve_fit(func, xdata, ydata, x0, sigma)[0]\n",
    "    plt.plot([x0[0], xFit[0]], [x0[1], xFit[1]], 'o-', ms=4,\n",
    "                 markeredgewidth=0, lw=2, color='pink')\n",
    "plt.colorbar(image)  # make colorbar\n",
    "plt.xlim(amin, amax)\n",
    "plt.ylim(bmin, bmax)\n",
    "plt.xlabel(r'$a$', fontsize=24)\n",
    "plt.ylabel(r'$b$', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: histogram fitting \n",
    "\n",
    "Here is an example of an unbinned *max-likelihood* fit of a set of events to a Gaussian PDF\n",
    "Courtesy http://glowingpython.blogspot.com/2012/07/distribution-fitting-with-scipy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from numpy import linspace\n",
    "from pylab import plot,show,hist,figure,title\n",
    "\n",
    "# generate 100 events from a normal distrubution\n",
    "# with mean 0 and standard deviation 1\n",
    "sample = norm.rvs(loc=0,scale=1,size=10000) \n",
    "\n",
    "title('Normal distribution')\n",
    "hist(sample,density=True,alpha=.3)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = norm.fit(sample) # distribution fitting\n",
    "\n",
    "#hist(sample,20)\n",
    "\n",
    "print (par)\n",
    "\n",
    "# now, par[0] and par[1] are the mean and \n",
    "# the standard deviation of the fitted distribution\n",
    "x = linspace(-5,5,100)\n",
    "# fitted distribution\n",
    "pdf_fitted = norm.pdf(x,loc=par[0],scale=par[1])\n",
    "# original distribution\n",
    "pdf = norm.pdf(x)\n",
    "\n",
    "title('Normal distribution')\n",
    "hist(sample,density=True,alpha=.3)\n",
    "plot(x,pdf_fitted,'r-',x,pdf,'b-')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Hypothesis testing\n",
    "\n",
    "Let's test if the distribution collected in class in Spring 2019 and Fall 2020 (poll of 71 students of any number from 0 to 9 inclusively) is consistent with a uniform distribution. How would we do it ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try #1: compute the likelihood for this dataset, i.e. the probability to observe exactly the dataset we observe, based on Poisson statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.loadtxt('sample_merged.dat')\n",
    "N = len(x)\n",
    "print('Number of samples =',N)\n",
    "\n",
    "bins = np.arange(-0.5,10.5,1)\n",
    "n, b, p = plt.hist(x, bins)\n",
    "expected = 0.1*N\n",
    "\n",
    "# loop over all entries in the histogram, compute chi^2 assuming \n",
    "# Poisson errors for each bin\n",
    "\n",
    "logL = 0\n",
    "for i in range(0,10):\n",
    "    value = n[i]\n",
    "    logL += sp.stats.poisson.logpmf(value,mu=expected)\n",
    "\n",
    "print('Log(likelihood) = ',logL)\n",
    "print('Probability = ',np.exp(logL))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the fact that the total probability is so small indicate that the hypothesis is invalid ? Not necessarily. We are looking at one particular outcome -- and the sheer number of all possible outcomes is large. So let's compute what a typical distribution of likelihood values is for a default hypothesis and 71 sampled students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrials = 10000\n",
    "logL_try = np.zeros(Ntrials)\n",
    "\n",
    "for itry in range(0,Ntrials):\n",
    "    # generate a random Poisson-distributed number for each bin\n",
    "    n = sp.stats.poisson.rvs(mu=expected, size=10)\n",
    "    logL_try[itry] = 0\n",
    "    for j in range(0,10):\n",
    "        value = n[j]\n",
    "        logL_try[itry] += sp.stats.poisson.logpmf(value,mu=expected)\n",
    "\n",
    "# plot the distribution\n",
    "entries, bins, patches = plt.hist(logL_try, 20)\n",
    "\n",
    "# compare the observed likelihood to the population\n",
    "pval = sp.stats.percentileofscore(logL_try,logL)\n",
    "print('p-value of logL={0:5.2f} is {1:5.1f}%'.format(logL,pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try #2: compute the $\\chi^2$ with respect to the uniform population hypothesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant function\n",
    "def model0(x, a):\n",
    "    return a\n",
    "\n",
    "vmodel0 = np.vectorize(model0) # make it take vectors\n",
    "\n",
    "# read data\n",
    "x = np.loadtxt('sample_merged.dat')\n",
    "N = len(x)\n",
    "print('Number of samples =',N)\n",
    "\n",
    "# make a histogram \n",
    "bins = np.arange(-0.5,10.5,1)\n",
    "n, b, p = plt.hist(x, bins)\n",
    "\n",
    "# fit the data to a constant\n",
    "xdata = [0.5*(b[i]+b[i+1]) for i in range(0,len(n))]\n",
    "en = [np.max([np.sqrt(val),1]) for val in n] # Poisson errors\n",
    "par, cov = fitter.curve_fit(model0, xdata, n, p0=[0.0], \n",
    "                            sigma=en, absolute_sigma=True)\n",
    "\n",
    "# compute chi^2  \n",
    "chi2 = np.sum(((n-vmodel0(xdata,*par))/en)**2)\n",
    "ndf = len(n)-1  # why -1 ? \n",
    "print('chi2 = {chi2:4.2f} for {ndf:d} df'.format(chi2=chi2,ndf=ndf))\n",
    "print('p-value = {0:4.2f}'.format(sp.stats.chi2.sf(chi2,ndf)))\n",
    "\n",
    "expected = 0.1*N*np.ones(len(n))\n",
    "# alternative way to compute p-value\n",
    "chi2_sp, p_sp = sp.stats.chisquare(f_obs=n,f_exp=expected)\n",
    "print('chi2 from stats package = {0:4.2f}'.format(chi2_sp))\n",
    "print('p-value from stats package = {0:4.2f}'.format(p_sp))\n",
    "\n",
    "# plot the distribition with error bars\n",
    "plt.errorbar(xdata, n, xerr=0, yerr=en, fmt='bo')\n",
    "plt.plot(xdata,vmodel0(xdata,*par),'r-')\n",
    "plt.xlabel('Number')\n",
    "plt.ylabel('Samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
